{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cd6385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rimsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "# from spellchecker import SpellChecker\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# spell = SpellChecker()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdee711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing urls\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub('', text)\n",
    "    # Removing special characters, punctuations, emojis and symbols\n",
    "    text = re.sub(r'[^\\w\\s\\d]|[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00026000-\\U00026FFF]', '', text)\n",
    "    # Removing email address \n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    # Removing newline\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Removing hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Removing whitespace and extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Removing stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # spelling correction\n",
    "#     corrected_tokens = [spell.correction(word) for word in filtered_tokens]\n",
    "    # lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "    return text\n",
    "\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    num_outputs = y_true.shape[1]\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        accuracy = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Overall accuracy is the average of accuracies for all outputs\n",
    "    overall_accuracy = sum(accuracies) / num_outputs\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "class CategoriesClassifier:\n",
    "    def __init__(self, df_train, df_test, df_validation):\n",
    "        self.df_train = df_train.copy()\n",
    "        self.df_test = df_test.copy()\n",
    "        self.df_validation = df_validation.copy()\n",
    "        self.data_type = {\n",
    "            'train': self.df_train,\n",
    "            'test': self.df_test,\n",
    "            'validation': self.df_validation,\n",
    "        }\n",
    "        \n",
    "    def preprocess_text(self, cols):\n",
    "        for v in self.data_type.values():\n",
    "            for c in cols:\n",
    "                v[c] = v[c].apply(transform_text)\n",
    "            \n",
    "    def visualize_top_categories(self, data, level, top_n=10):\n",
    "        df = self.data_type.get(data)\n",
    "        top_categories = df[level].value_counts().nlargest(top_n)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=top_categories.values, y=top_categories.index, palette='viridis')\n",
    "        plt.title(f'Top {top_n} Categories in {level}')\n",
    "        plt.xlabel('Number of Products')\n",
    "        plt.ylabel('Category')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_wordcloud(self, data):\n",
    "        df = self.data_type.get(data)\n",
    "        all_descriptions = ' '.join(df['Description'])\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Product Descriptions\\n')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_description_length(self, data):\n",
    "        df = self.data_type.get(data)\n",
    "        df['Description_Length'] = df['Description'].apply(len)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(df['Description_Length'], bins=20, kde=True)\n",
    "        plt.xlabel('Description Length')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Description Length Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "    def apply_tfidf(self, data, col):\n",
    "        df = self.data_type.get(data)\n",
    "        tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "        return tfidf.fit_transform(df[col]).toarray()\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        # Target encoding\n",
    "        self.target_encoding = {\n",
    "            'lvl1': {v: i for i, v in enumerate(self.df_train['lvl1'].unique())},\n",
    "            'lvl2': {v: i for i, v in enumerate(self.df_train['lvl2'].unique())},\n",
    "            'lvl3': {v: i for i, v in enumerate(self.df_train['lvl3'].unique())}\n",
    "        }\n",
    "\n",
    "        # Create a TF-IDF vectorizer and fit it on the training data\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, ngram_range=(1, 2), stop_words='english')\n",
    "        self.X_train = self.tfidf_vectorizer.fit_transform(self.df_train['Description']).toarray()\n",
    "\n",
    "        # Use the same vectorizer to transform validation and test data\n",
    "        self.X_val = self.tfidf_vectorizer.transform(self.df_validation['Description']).toarray()\n",
    "        self.X_test = self.tfidf_vectorizer.transform(self.df_test['Description']).toarray()\n",
    "\n",
    "\n",
    "        # Target encoding for y values\n",
    "        self.y_train_lvl1 = self.df_train['lvl1'].map(self.target_encoding.get('lvl1')).values\n",
    "        self.y_train_lvl2 = self.df_train['lvl2'].map(self.target_encoding.get('lvl2')).values\n",
    "        self.y_train_lvl3 = self.df_train['lvl3'].map(self.target_encoding.get('lvl3')).values\n",
    "\n",
    "        self.y_val_lvl1 = self.df_validation['lvl1'].map(self.target_encoding.get('lvl1')).values\n",
    "        self.y_val_lvl2 = self.df_validation['lvl2'].map(self.target_encoding.get('lvl2')).values\n",
    "        self.y_val_lvl3 = self.df_validation['lvl3'].map(self.target_encoding.get('lvl3')).values\n",
    "\n",
    "        # Stack them horizontally to create y_train\n",
    "        self.y_train = np.column_stack((self.y_train_lvl1, self.y_train_lvl2, self.y_train_lvl3))\n",
    "\n",
    "        # Stack them horizontally to create y_val\n",
    "        self.y_val = np.column_stack((self.y_val_lvl1, self.y_val_lvl2, self.y_val_lvl3))\n",
    "\n",
    "        \n",
    "    def train_evaluate_ml_models(self, models, param_grids):\n",
    "        # Train and evaluate each model\n",
    "        best_models = {}\n",
    "        accuracies = {}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Training and evaluating {model_name}...\")\n",
    "            param_grid = param_grids[model_name]\n",
    "            multi_output_model = MultiOutputClassifier(model)\n",
    "            grid_search = GridSearchCV(multi_output_model, param_grid, cv=3, n_jobs=-1)\n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # Save the best model\n",
    "            best_models[model_name] = grid_search.best_estimator_\n",
    "\n",
    "            # Predict on train and validation sets\n",
    "            y_train_pred = grid_search.predict(self.X_train)\n",
    "            y_val_pred = grid_search.predict(self.X_val)\n",
    "            \n",
    "\n",
    "            # Calculate accuracy for train and validation sets\n",
    "            train_accuracy = multioutput_accuracy(self.y_train, y_train_pred)\n",
    "            val_accuracy = multioutput_accuracy(self.y_val, y_val_pred)\n",
    "\n",
    "            accuracies[model_name] = {'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy}\n",
    "\n",
    "            print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "            print(f\"Train accuracy for {model_name}: {train_accuracy}\")\n",
    "            print(f\"Validation accuracy for {model_name}: {val_accuracy}\")\n",
    "\n",
    "        return best_models, accuracies\n",
    "#     # Rest of the functions remain the same...\n",
    "\n",
    "# # Usage:\n",
    "# # Create the classifier instance\n",
    "# classifier = CategoriesClassifier(df_train, df_test, df_validation)\n",
    "\n",
    "# # Preprocess the 'Description' column\n",
    "# classifier.preprocess_data('Description')\n",
    "\n",
    "# # Vectorize the data\n",
    "# classifier.vectorize_data()\n",
    "\n",
    "# # Train and evaluate the multi-output models\n",
    "# best_models = classifier.train_evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c432ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('Data/train_data.json', lines=True)\n",
    "df_test = pd.read_json('Data/test_data.json', lines=True)\n",
    "df_validation = pd.read_json('Data/validation_data.json', lines=True)\n",
    "\n",
    "cls = CategoriesClassifier(df_train, df_test, df_validation)\n",
    "cls.preprocess_text(['Description', 'Name', 'CategoryText'])\n",
    "# cls.visualize_top_categories('train', 'lvl1')\n",
    "# cls.visualize_top_categories('train', 'lvl2')\n",
    "# cls.visualize_top_categories('train', 'lvl3')\n",
    "# cls.visualize_wordcloud('train')\n",
    "# cls.visualize_description_length('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e067d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051428d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating RandomForest...\n"
     ]
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'KNeighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Parameter grids for grid search\n",
    "param_grids = {\n",
    "    'RandomForest': {'estimator__n_estimators': [50, 100, 200]},\n",
    "    'SVM': {'estimator__C': [0.1, 1, 10]},\n",
    "    'KNeighbors': {'estimator__n_neighbors': [3, 5, 7]}\n",
    "}\n",
    "\n",
    "best_models, accuracies = cls.train_evaluate_ml_models(models, param_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "14759bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e441298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import unicodedata\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# def correct_spelling(text):\n",
    "#     # Correct spelling errors using a spell checker\n",
    "#     spell = SpellChecker()\n",
    "#     tokens = word_tokenize(text)\n",
    "#     corrected_tokens = [spell.correction(word) for word in tokens]\n",
    "#     return ' '.join(corrected_tokens)\n",
    "\n",
    "# def remove_html_tags(text):\n",
    "#     # Remove HTML tags from the text\n",
    "#     cleaner = re.compile('<.*?>')\n",
    "#     cleaned_text = re.sub(cleaner, '', text)\n",
    "#     return cleaned_text\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     # Remove stopwords using spaCy (which has a more comprehensive stopwords list)\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.text for token in doc if not token.is_stop]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# import string\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     # Remove punctuation\n",
    "#     translator = str.maketrans('', '', string.punctuation)\n",
    "#     return text.translate(translator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "840ff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import hamming_loss, jaccard_score, f1_score\n",
    "\n",
    "# # Load the data into a DataFrame\n",
    "# data = [...]  # Replace with your data\n",
    "# df = df_train.copy()\n",
    "\n",
    "# # Feature Engineering\n",
    "# X = df['Description']  # Feature: Description\n",
    "# y = df[['lvl1', 'lvl2', 'lvl3']]  # Targets: lvl1, lvl2, lvl3\n",
    "\n",
    "# # Convert Description to TF-IDF features\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Model Selection and Training\n",
    "# models = []\n",
    "# for i in range(y.shape[1]):\n",
    "#     model = LogisticRegression()\n",
    "#     model.fit(X_train, y_train.iloc[:, i])\n",
    "#     models.append(model)\n",
    "\n",
    "# # Model Evaluation\n",
    "# y_pred = []\n",
    "# for model in models:\n",
    "#     y_pred.append(model.predict(X_test))\n",
    "\n",
    "# y_pred = pd.DataFrame(y_pred).T\n",
    "\n",
    "# print('Hamming Loss:', hamming_loss(y_test, y_pred))\n",
    "# print('Jaccard Score:', jaccard_score(y_test, y_pred, average='samples'))\n",
    "# print('F1 Score:', f1_score(y_test, y_pred, average='samples'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
